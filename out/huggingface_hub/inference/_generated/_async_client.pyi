import numpy as np
from PIL.Image import Image as Image
from _typeshed import Incomplete
from aiohttp import ClientResponse as ClientResponse, ClientSession as ClientSession
from huggingface_hub.constants import ALL_INFERENCE_API_FRAMEWORKS as ALL_INFERENCE_API_FRAMEWORKS, INFERENCE_ENDPOINT as INFERENCE_ENDPOINT, MAIN_INFERENCE_API_FRAMEWORKS as MAIN_INFERENCE_API_FRAMEWORKS
from huggingface_hub.errors import InferenceTimeoutError as InferenceTimeoutError
from huggingface_hub.inference._common import ContentT as ContentT, ModelStatus as ModelStatus, TASKS_EXPECTING_IMAGES as TASKS_EXPECTING_IMAGES, raise_text_generation_error as raise_text_generation_error
from huggingface_hub.inference._generated.types import AudioClassificationOutputElement as AudioClassificationOutputElement, AudioClassificationOutputTransform as AudioClassificationOutputTransform, AudioToAudioOutputElement as AudioToAudioOutputElement, AutomaticSpeechRecognitionOutput as AutomaticSpeechRecognitionOutput, ChatCompletionInputGrammarType as ChatCompletionInputGrammarType, ChatCompletionInputStreamOptions as ChatCompletionInputStreamOptions, ChatCompletionInputToolType as ChatCompletionInputToolType, ChatCompletionOutput as ChatCompletionOutput, ChatCompletionStreamOutput as ChatCompletionStreamOutput, DocumentQuestionAnsweringOutputElement as DocumentQuestionAnsweringOutputElement, FillMaskOutputElement as FillMaskOutputElement, ImageClassificationOutputElement as ImageClassificationOutputElement, ImageSegmentationOutputElement as ImageSegmentationOutputElement, ImageToTextOutput as ImageToTextOutput, ObjectDetectionOutputElement as ObjectDetectionOutputElement, QuestionAnsweringOutputElement as QuestionAnsweringOutputElement, SummarizationOutput as SummarizationOutput, TableQuestionAnsweringOutputElement as TableQuestionAnsweringOutputElement, TextClassificationOutputElement as TextClassificationOutputElement, TextClassificationOutputTransform as TextClassificationOutputTransform, TextGenerationInputGrammarType as TextGenerationInputGrammarType, TextGenerationOutput as TextGenerationOutput, TextGenerationStreamOutput as TextGenerationStreamOutput, TextToImageTargetSize as TextToImageTargetSize, TextToSpeechEarlyStoppingEnum as TextToSpeechEarlyStoppingEnum, TokenClassificationOutputElement as TokenClassificationOutputElement, ToolElement as ToolElement, TranslationOutput as TranslationOutput, VisualQuestionAnsweringOutputElement as VisualQuestionAnsweringOutputElement, ZeroShotClassificationOutputElement as ZeroShotClassificationOutputElement, ZeroShotImageClassificationOutputElement as ZeroShotImageClassificationOutputElement
from huggingface_hub.utils import build_hf_headers as build_hf_headers
from typing import Any, AsyncIterable, Literal, overload

logger: Incomplete
MODEL_KWARGS_NOT_USED_REGEX: Incomplete

class AsyncInferenceClient:
    model: Incomplete
    token: Incomplete
    headers: Incomplete
    cookies: Incomplete
    timeout: Incomplete
    trust_env: Incomplete
    proxies: Incomplete
    base_url: Incomplete
    def __init__(self, model: str | None = None, *, token: str | bool | None = None, timeout: float | None = None, headers: dict[str, str] | None = None, cookies: dict[str, str] | None = None, trust_env: bool = False, proxies: Any | None = None, base_url: str | None = None, api_key: str | None = None) -> None: ...
    @overload
    async def post(self, *, json: str | dict | list | None = None, data: ContentT | None = None, model: str | None = None, task: str | None = None, stream: Literal[False] = ...) -> bytes: ...
    @overload
    async def post(self, *, json: str | dict | list | None = None, data: ContentT | None = None, model: str | None = None, task: str | None = None, stream: Literal[True] = ...) -> AsyncIterable[bytes]: ...
    @overload
    async def post(self, *, json: str | dict | list | None = None, data: ContentT | None = None, model: str | None = None, task: str | None = None, stream: bool = False) -> bytes | AsyncIterable[bytes]: ...
    async def __aenter__(self): ...
    async def __aexit__(self, exc_type, exc_value, traceback) -> None: ...
    def __del__(self) -> None: ...
    async def close(self) -> None: ...
    async def audio_classification(self, audio: ContentT, *, model: str | None = None, top_k: int | None = None, function_to_apply: AudioClassificationOutputTransform | None = None) -> list[AudioClassificationOutputElement]: ...
    async def audio_to_audio(self, audio: ContentT, *, model: str | None = None) -> list[AudioToAudioOutputElement]: ...
    async def automatic_speech_recognition(self, audio: ContentT, *, model: str | None = None) -> AutomaticSpeechRecognitionOutput: ...
    @overload
    async def chat_completion(self, messages: list[dict], *, model: str | None = None, stream: Literal[False] = False, frequency_penalty: float | None = None, logit_bias: list[float] | None = None, logprobs: bool | None = None, max_tokens: int | None = None, n: int | None = None, presence_penalty: float | None = None, response_format: ChatCompletionInputGrammarType | None = None, seed: int | None = None, stop: list[str] | None = None, stream_options: ChatCompletionInputStreamOptions | None = None, temperature: float | None = None, tool_choice: ChatCompletionInputToolType | str | None = None, tool_prompt: str | None = None, tools: list[ToolElement] | None = None, top_logprobs: int | None = None, top_p: float | None = None) -> ChatCompletionOutput: ...
    @overload
    async def chat_completion(self, messages: list[dict], *, model: str | None = None, stream: Literal[True] = True, frequency_penalty: float | None = None, logit_bias: list[float] | None = None, logprobs: bool | None = None, max_tokens: int | None = None, n: int | None = None, presence_penalty: float | None = None, response_format: ChatCompletionInputGrammarType | None = None, seed: int | None = None, stop: list[str] | None = None, stream_options: ChatCompletionInputStreamOptions | None = None, temperature: float | None = None, tool_choice: ChatCompletionInputToolType | str | None = None, tool_prompt: str | None = None, tools: list[ToolElement] | None = None, top_logprobs: int | None = None, top_p: float | None = None) -> AsyncIterable[ChatCompletionStreamOutput]: ...
    @overload
    async def chat_completion(self, messages: list[dict], *, model: str | None = None, stream: bool = False, frequency_penalty: float | None = None, logit_bias: list[float] | None = None, logprobs: bool | None = None, max_tokens: int | None = None, n: int | None = None, presence_penalty: float | None = None, response_format: ChatCompletionInputGrammarType | None = None, seed: int | None = None, stop: list[str] | None = None, stream_options: ChatCompletionInputStreamOptions | None = None, temperature: float | None = None, tool_choice: ChatCompletionInputToolType | str | None = None, tool_prompt: str | None = None, tools: list[ToolElement] | None = None, top_logprobs: int | None = None, top_p: float | None = None) -> ChatCompletionOutput | AsyncIterable[ChatCompletionStreamOutput]: ...
    async def document_question_answering(self, image: ContentT, question: str, *, model: str | None = None, doc_stride: int | None = None, handle_impossible_answer: bool | None = None, lang: str | None = None, max_answer_len: int | None = None, max_question_len: int | None = None, max_seq_len: int | None = None, top_k: int | None = None, word_boxes: list[list[float] | str] | None = None) -> list[DocumentQuestionAnsweringOutputElement]: ...
    async def feature_extraction(self, text: str, *, normalize: bool | None = None, prompt_name: str | None = None, truncate: bool | None = None, truncation_direction: Literal['Left', 'Right'] | None = None, model: str | None = None) -> np.ndarray: ...
    async def fill_mask(self, text: str, *, model: str | None = None, targets: list[str] | None = None, top_k: int | None = None) -> list[FillMaskOutputElement]: ...
    async def image_classification(self, image: ContentT, *, model: str | None = None, function_to_apply: Literal['sigmoid', 'softmax', 'none'] | None = None, top_k: int | None = None) -> list[ImageClassificationOutputElement]: ...
    async def image_segmentation(self, image: ContentT, *, model: str | None = None, mask_threshold: float | None = None, overlap_mask_area_threshold: float | None = None, subtask: Literal['instance', 'panoptic', 'semantic'] | None = None, threshold: float | None = None) -> list[ImageSegmentationOutputElement]: ...
    async def image_to_image(self, image: ContentT, prompt: str | None = None, *, negative_prompt: str | None = None, height: int | None = None, width: int | None = None, num_inference_steps: int | None = None, guidance_scale: float | None = None, model: str | None = None, **kwargs) -> Image: ...
    async def image_to_text(self, image: ContentT, *, model: str | None = None) -> ImageToTextOutput: ...
    async def list_deployed_models(self, frameworks: None | str | Literal['all'] | list[str] = None) -> dict[str, list[str]]: ...
    async def object_detection(self, image: ContentT, *, model: str | None = None, threshold: float | None = None) -> list[ObjectDetectionOutputElement]: ...
    async def question_answering(self, question: str, context: str, *, model: str | None = None, align_to_words: bool | None = None, doc_stride: int | None = None, handle_impossible_answer: bool | None = None, max_answer_len: int | None = None, max_question_len: int | None = None, max_seq_len: int | None = None, top_k: int | None = None) -> QuestionAnsweringOutputElement | list[QuestionAnsweringOutputElement]: ...
    async def sentence_similarity(self, sentence: str, other_sentences: list[str], *, model: str | None = None) -> list[float]: ...
    async def summarization(self, text: str, *, parameters: dict[str, Any] | None = None, model: str | None = None, clean_up_tokenization_spaces: bool | None = None, generate_parameters: dict[str, Any] | None = None, truncation: Literal['do_not_truncate', 'longest_first', 'only_first', 'only_second'] | None = None) -> SummarizationOutput: ...
    async def table_question_answering(self, table: dict[str, Any], query: str, *, model: str | None = None, parameters: dict[str, Any] | None = None) -> TableQuestionAnsweringOutputElement: ...
    async def tabular_classification(self, table: dict[str, Any], *, model: str | None = None) -> list[str]: ...
    async def tabular_regression(self, table: dict[str, Any], *, model: str | None = None) -> list[float]: ...
    async def text_classification(self, text: str, *, model: str | None = None, top_k: int | None = None, function_to_apply: TextClassificationOutputTransform | None = None) -> list[TextClassificationOutputElement]: ...
    @overload
    async def text_generation(self, prompt: str, *, details: Literal[False] = ..., stream: Literal[False] = ..., model: str | None = None, adapter_id: str | None = None, best_of: int | None = None, decoder_input_details: bool | None = None, do_sample: bool | None = False, frequency_penalty: float | None = None, grammar: TextGenerationInputGrammarType | None = None, max_new_tokens: int | None = None, repetition_penalty: float | None = None, return_full_text: bool | None = False, seed: int | None = None, stop: list[str] | None = None, stop_sequences: list[str] | None = None, temperature: float | None = None, top_k: int | None = None, top_n_tokens: int | None = None, top_p: float | None = None, truncate: int | None = None, typical_p: float | None = None, watermark: bool | None = None) -> str: ...
    @overload
    async def text_generation(self, prompt: str, *, details: Literal[True] = ..., stream: Literal[False] = ..., model: str | None = None, adapter_id: str | None = None, best_of: int | None = None, decoder_input_details: bool | None = None, do_sample: bool | None = False, frequency_penalty: float | None = None, grammar: TextGenerationInputGrammarType | None = None, max_new_tokens: int | None = None, repetition_penalty: float | None = None, return_full_text: bool | None = False, seed: int | None = None, stop: list[str] | None = None, stop_sequences: list[str] | None = None, temperature: float | None = None, top_k: int | None = None, top_n_tokens: int | None = None, top_p: float | None = None, truncate: int | None = None, typical_p: float | None = None, watermark: bool | None = None) -> TextGenerationOutput: ...
    @overload
    async def text_generation(self, prompt: str, *, details: Literal[False] = ..., stream: Literal[True] = ..., model: str | None = None, adapter_id: str | None = None, best_of: int | None = None, decoder_input_details: bool | None = None, do_sample: bool | None = False, frequency_penalty: float | None = None, grammar: TextGenerationInputGrammarType | None = None, max_new_tokens: int | None = None, repetition_penalty: float | None = None, return_full_text: bool | None = False, seed: int | None = None, stop: list[str] | None = None, stop_sequences: list[str] | None = None, temperature: float | None = None, top_k: int | None = None, top_n_tokens: int | None = None, top_p: float | None = None, truncate: int | None = None, typical_p: float | None = None, watermark: bool | None = None) -> AsyncIterable[str]: ...
    @overload
    async def text_generation(self, prompt: str, *, details: Literal[True] = ..., stream: Literal[True] = ..., model: str | None = None, adapter_id: str | None = None, best_of: int | None = None, decoder_input_details: bool | None = None, do_sample: bool | None = False, frequency_penalty: float | None = None, grammar: TextGenerationInputGrammarType | None = None, max_new_tokens: int | None = None, repetition_penalty: float | None = None, return_full_text: bool | None = False, seed: int | None = None, stop: list[str] | None = None, stop_sequences: list[str] | None = None, temperature: float | None = None, top_k: int | None = None, top_n_tokens: int | None = None, top_p: float | None = None, truncate: int | None = None, typical_p: float | None = None, watermark: bool | None = None) -> AsyncIterable[TextGenerationStreamOutput]: ...
    @overload
    async def text_generation(self, prompt: str, *, details: Literal[True] = ..., stream: bool = ..., model: str | None = None, adapter_id: str | None = None, best_of: int | None = None, decoder_input_details: bool | None = None, do_sample: bool | None = False, frequency_penalty: float | None = None, grammar: TextGenerationInputGrammarType | None = None, max_new_tokens: int | None = None, repetition_penalty: float | None = None, return_full_text: bool | None = False, seed: int | None = None, stop: list[str] | None = None, stop_sequences: list[str] | None = None, temperature: float | None = None, top_k: int | None = None, top_n_tokens: int | None = None, top_p: float | None = None, truncate: int | None = None, typical_p: float | None = None, watermark: bool | None = None) -> TextGenerationOutput | AsyncIterable[TextGenerationStreamOutput]: ...
    async def text_to_image(self, prompt: str, *, negative_prompt: str | None = None, height: float | None = None, width: float | None = None, num_inference_steps: float | None = None, guidance_scale: float | None = None, model: str | None = None, scheduler: str | None = None, target_size: TextToImageTargetSize | None = None, seed: int | None = None, **kwargs) -> Image: ...
    async def text_to_speech(self, text: str, *, model: str | None = None, do_sample: bool | None = None, early_stopping: bool | TextToSpeechEarlyStoppingEnum | None = None, epsilon_cutoff: float | None = None, eta_cutoff: float | None = None, max_length: int | None = None, max_new_tokens: int | None = None, min_length: int | None = None, min_new_tokens: int | None = None, num_beam_groups: int | None = None, num_beams: int | None = None, penalty_alpha: float | None = None, temperature: float | None = None, top_k: int | None = None, top_p: float | None = None, typical_p: float | None = None, use_cache: bool | None = None) -> bytes: ...
    async def token_classification(self, text: str, *, model: str | None = None, aggregation_strategy: Literal['none', 'simple', 'first', 'average', 'max'] | None = None, ignore_labels: list[str] | None = None, stride: int | None = None) -> list[TokenClassificationOutputElement]: ...
    async def translation(self, text: str, *, model: str | None = None, src_lang: str | None = None, tgt_lang: str | None = None, clean_up_tokenization_spaces: bool | None = None, truncation: Literal['do_not_truncate', 'longest_first', 'only_first', 'only_second'] | None = None, generate_parameters: dict[str, Any] | None = None) -> TranslationOutput: ...
    async def visual_question_answering(self, image: ContentT, question: str, *, model: str | None = None, top_k: int | None = None) -> list[VisualQuestionAnsweringOutputElement]: ...
    async def zero_shot_classification(self, text: str, labels: list[str], *, multi_label: bool = False, hypothesis_template: str | None = None, model: str | None = None) -> list[ZeroShotClassificationOutputElement]: ...
    async def zero_shot_image_classification(self, image: ContentT, labels: list[str], *, model: str | None = None, hypothesis_template: str | None = None) -> list[ZeroShotImageClassificationOutputElement]: ...
    @staticmethod
    def get_recommended_model(task: str) -> str: ...
    async def get_endpoint_info(self, *, model: str | None = None) -> dict[str, Any]: ...
    async def health_check(self, model: str | None = None) -> bool: ...
    async def get_model_status(self, model: str | None = None) -> ModelStatus: ...
    @property
    def chat(self) -> ProxyClientChat: ...

class _ProxyClient:
    def __init__(self, client: AsyncInferenceClient) -> None: ...

class ProxyClientChat(_ProxyClient):
    @property
    def completions(self) -> ProxyClientChatCompletions: ...

class ProxyClientChatCompletions(_ProxyClient):
    @property
    def create(self): ...
