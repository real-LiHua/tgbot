import torch
from .. import constants as constants, logging as logging
from ._base import MAX_SHARD_SIZE as MAX_SHARD_SIZE, StateDictSplit as StateDictSplit, split_state_dict_into_shards_factory as split_state_dict_into_shards_factory
from _typeshed import Incomplete
from pathlib import Path
from typing import Any

logger: Incomplete

def save_torch_model(model: torch.nn.Module, save_directory: str | Path, *, filename_pattern: str | None = None, force_contiguous: bool = True, max_shard_size: int | str = ..., metadata: dict[str, str] | None = None, safe_serialization: bool = True, shared_tensors_to_discard: list[str] | None = None): ...
def save_torch_state_dict(state_dict: dict[str, 'torch.Tensor'], save_directory: str | Path, *, filename_pattern: str | None = None, force_contiguous: bool = True, max_shard_size: int | str = ..., metadata: dict[str, str] | None = None, safe_serialization: bool = True, shared_tensors_to_discard: list[str] | None = None) -> None: ...
def split_torch_state_dict_into_shards(state_dict: dict[str, 'torch.Tensor'], *, filename_pattern: str = ..., max_shard_size: int | str = ...) -> StateDictSplit: ...
def get_torch_storage_id(tensor: torch.Tensor) -> tuple['torch.device', int | tuple[Any, ...], int] | None: ...
def get_torch_storage_size(tensor: torch.Tensor) -> int: ...
def is_torch_tpu_available(check_device: bool = True): ...
def storage_ptr(tensor: torch.Tensor) -> int | tuple[Any, ...]: ...
