from .hf_api import HfApi as HfApi
from .inference._client import InferenceClient as InferenceClient
from .inference._generated._async_client import AsyncInferenceClient as AsyncInferenceClient
from .utils import get_session as get_session, logging as logging, parse_datetime as parse_datetime
from _typeshed import Incomplete
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from huggingface_hub.errors import InferenceEndpointError as InferenceEndpointError, InferenceEndpointTimeoutError as InferenceEndpointTimeoutError

logger: Incomplete

class InferenceEndpointStatus(str, Enum):
    PENDING = 'pending'
    INITIALIZING = 'initializing'
    UPDATING = 'updating'
    UPDATE_FAILED = 'updateFailed'
    RUNNING = 'running'
    PAUSED = 'paused'
    FAILED = 'failed'
    SCALED_TO_ZERO = 'scaledToZero'

class InferenceEndpointType(str, Enum):
    PUBlIC = 'public'
    PROTECTED = 'protected'
    PRIVATE = 'private'

@dataclass
class InferenceEndpoint:
    name: str = ...
    namespace: str
    repository: str = ...
    status: InferenceEndpointStatus = ...
    url: str | None = ...
    framework: str = ...
    revision: str = ...
    task: str = ...
    created_at: datetime = ...
    updated_at: datetime = ...
    type: InferenceEndpointType = ...
    raw: dict = ...
    @classmethod
    def from_raw(cls, raw: dict, namespace: str, token: str | bool | None = None, api: HfApi | None = None) -> InferenceEndpoint: ...
    def __post_init__(self) -> None: ...
    @property
    def client(self) -> InferenceClient: ...
    @property
    def async_client(self) -> AsyncInferenceClient: ...
    def wait(self, timeout: int | None = None, refresh_every: int = 5) -> InferenceEndpoint: ...
    def fetch(self) -> InferenceEndpoint: ...
    def update(self, *, accelerator: str | None = None, instance_size: str | None = None, instance_type: str | None = None, min_replica: int | None = None, max_replica: int | None = None, scale_to_zero_timeout: int | None = None, repository: str | None = None, framework: str | None = None, revision: str | None = None, task: str | None = None, custom_image: dict | None = None, secrets: dict[str, str] | None = None) -> InferenceEndpoint: ...
    def pause(self) -> InferenceEndpoint: ...
    def resume(self, running_ok: bool = True) -> InferenceEndpoint: ...
    def scale_to_zero(self) -> InferenceEndpoint: ...
    def delete(self) -> None: ...
    def __init__(self, namespace, raw, _token, _api) -> None: ...
