import numpy as np
from PIL.Image import Image as Image
from _typeshed import Incomplete
from huggingface_hub.constants import ALL_INFERENCE_API_FRAMEWORKS as ALL_INFERENCE_API_FRAMEWORKS, INFERENCE_ENDPOINT as INFERENCE_ENDPOINT, MAIN_INFERENCE_API_FRAMEWORKS as MAIN_INFERENCE_API_FRAMEWORKS
from huggingface_hub.errors import BadRequestError as BadRequestError, InferenceTimeoutError as InferenceTimeoutError
from huggingface_hub.inference._common import ContentT as ContentT, ModelStatus as ModelStatus, TASKS_EXPECTING_IMAGES as TASKS_EXPECTING_IMAGES, raise_text_generation_error as raise_text_generation_error
from huggingface_hub.inference._generated.types import AudioClassificationOutputElement as AudioClassificationOutputElement, AudioClassificationOutputTransform as AudioClassificationOutputTransform, AudioToAudioOutputElement as AudioToAudioOutputElement, AutomaticSpeechRecognitionOutput as AutomaticSpeechRecognitionOutput, ChatCompletionInputGrammarType as ChatCompletionInputGrammarType, ChatCompletionInputStreamOptions as ChatCompletionInputStreamOptions, ChatCompletionInputToolType as ChatCompletionInputToolType, ChatCompletionOutput as ChatCompletionOutput, ChatCompletionStreamOutput as ChatCompletionStreamOutput, DocumentQuestionAnsweringOutputElement as DocumentQuestionAnsweringOutputElement, FillMaskOutputElement as FillMaskOutputElement, ImageClassificationOutputElement as ImageClassificationOutputElement, ImageSegmentationOutputElement as ImageSegmentationOutputElement, ImageToTextOutput as ImageToTextOutput, ObjectDetectionOutputElement as ObjectDetectionOutputElement, QuestionAnsweringOutputElement as QuestionAnsweringOutputElement, SummarizationOutput as SummarizationOutput, TableQuestionAnsweringOutputElement as TableQuestionAnsweringOutputElement, TextClassificationOutputElement as TextClassificationOutputElement, TextClassificationOutputTransform as TextClassificationOutputTransform, TextGenerationInputGrammarType as TextGenerationInputGrammarType, TextGenerationOutput as TextGenerationOutput, TextGenerationStreamOutput as TextGenerationStreamOutput, TextToImageTargetSize as TextToImageTargetSize, TextToSpeechEarlyStoppingEnum as TextToSpeechEarlyStoppingEnum, TokenClassificationOutputElement as TokenClassificationOutputElement, ToolElement as ToolElement, TranslationOutput as TranslationOutput, VisualQuestionAnsweringOutputElement as VisualQuestionAnsweringOutputElement, ZeroShotClassificationOutputElement as ZeroShotClassificationOutputElement, ZeroShotImageClassificationOutputElement as ZeroShotImageClassificationOutputElement
from huggingface_hub.utils import build_hf_headers as build_hf_headers, get_session as get_session, hf_raise_for_status as hf_raise_for_status
from typing import Any, Iterable, Literal, overload

logger: Incomplete
MODEL_KWARGS_NOT_USED_REGEX: Incomplete

class InferenceClient:
    model: Incomplete
    token: Incomplete
    headers: Incomplete
    cookies: Incomplete
    timeout: Incomplete
    proxies: Incomplete
    base_url: Incomplete
    def __init__(self, model: str | None = None, *, token: str | bool | None = None, timeout: float | None = None, headers: dict[str, str] | None = None, cookies: dict[str, str] | None = None, proxies: Any | None = None, base_url: str | None = None, api_key: str | None = None) -> None: ...
    @overload
    def post(self, *, json: str | dict | list | None = None, data: ContentT | None = None, model: str | None = None, task: str | None = None, stream: Literal[False] = ...) -> bytes: ...
    @overload
    def post(self, *, json: str | dict | list | None = None, data: ContentT | None = None, model: str | None = None, task: str | None = None, stream: Literal[True] = ...) -> Iterable[bytes]: ...
    @overload
    def post(self, *, json: str | dict | list | None = None, data: ContentT | None = None, model: str | None = None, task: str | None = None, stream: bool = False) -> bytes | Iterable[bytes]: ...
    def audio_classification(self, audio: ContentT, *, model: str | None = None, top_k: int | None = None, function_to_apply: AudioClassificationOutputTransform | None = None) -> list[AudioClassificationOutputElement]: ...
    def audio_to_audio(self, audio: ContentT, *, model: str | None = None) -> list[AudioToAudioOutputElement]: ...
    def automatic_speech_recognition(self, audio: ContentT, *, model: str | None = None) -> AutomaticSpeechRecognitionOutput: ...
    @overload
    def chat_completion(self, messages: list[dict], *, model: str | None = None, stream: Literal[False] = False, frequency_penalty: float | None = None, logit_bias: list[float] | None = None, logprobs: bool | None = None, max_tokens: int | None = None, n: int | None = None, presence_penalty: float | None = None, response_format: ChatCompletionInputGrammarType | None = None, seed: int | None = None, stop: list[str] | None = None, stream_options: ChatCompletionInputStreamOptions | None = None, temperature: float | None = None, tool_choice: ChatCompletionInputToolType | str | None = None, tool_prompt: str | None = None, tools: list[ToolElement] | None = None, top_logprobs: int | None = None, top_p: float | None = None) -> ChatCompletionOutput: ...
    @overload
    def chat_completion(self, messages: list[dict], *, model: str | None = None, stream: Literal[True] = True, frequency_penalty: float | None = None, logit_bias: list[float] | None = None, logprobs: bool | None = None, max_tokens: int | None = None, n: int | None = None, presence_penalty: float | None = None, response_format: ChatCompletionInputGrammarType | None = None, seed: int | None = None, stop: list[str] | None = None, stream_options: ChatCompletionInputStreamOptions | None = None, temperature: float | None = None, tool_choice: ChatCompletionInputToolType | str | None = None, tool_prompt: str | None = None, tools: list[ToolElement] | None = None, top_logprobs: int | None = None, top_p: float | None = None) -> Iterable[ChatCompletionStreamOutput]: ...
    @overload
    def chat_completion(self, messages: list[dict], *, model: str | None = None, stream: bool = False, frequency_penalty: float | None = None, logit_bias: list[float] | None = None, logprobs: bool | None = None, max_tokens: int | None = None, n: int | None = None, presence_penalty: float | None = None, response_format: ChatCompletionInputGrammarType | None = None, seed: int | None = None, stop: list[str] | None = None, stream_options: ChatCompletionInputStreamOptions | None = None, temperature: float | None = None, tool_choice: ChatCompletionInputToolType | str | None = None, tool_prompt: str | None = None, tools: list[ToolElement] | None = None, top_logprobs: int | None = None, top_p: float | None = None) -> ChatCompletionOutput | Iterable[ChatCompletionStreamOutput]: ...
    def document_question_answering(self, image: ContentT, question: str, *, model: str | None = None, doc_stride: int | None = None, handle_impossible_answer: bool | None = None, lang: str | None = None, max_answer_len: int | None = None, max_question_len: int | None = None, max_seq_len: int | None = None, top_k: int | None = None, word_boxes: list[list[float] | str] | None = None) -> list[DocumentQuestionAnsweringOutputElement]: ...
    def feature_extraction(self, text: str, *, normalize: bool | None = None, prompt_name: str | None = None, truncate: bool | None = None, truncation_direction: Literal['Left', 'Right'] | None = None, model: str | None = None) -> np.ndarray: ...
    def fill_mask(self, text: str, *, model: str | None = None, targets: list[str] | None = None, top_k: int | None = None) -> list[FillMaskOutputElement]: ...
    def image_classification(self, image: ContentT, *, model: str | None = None, function_to_apply: Literal['sigmoid', 'softmax', 'none'] | None = None, top_k: int | None = None) -> list[ImageClassificationOutputElement]: ...
    def image_segmentation(self, image: ContentT, *, model: str | None = None, mask_threshold: float | None = None, overlap_mask_area_threshold: float | None = None, subtask: Literal['instance', 'panoptic', 'semantic'] | None = None, threshold: float | None = None) -> list[ImageSegmentationOutputElement]: ...
    def image_to_image(self, image: ContentT, prompt: str | None = None, *, negative_prompt: str | None = None, height: int | None = None, width: int | None = None, num_inference_steps: int | None = None, guidance_scale: float | None = None, model: str | None = None, **kwargs) -> Image: ...
    def image_to_text(self, image: ContentT, *, model: str | None = None) -> ImageToTextOutput: ...
    def list_deployed_models(self, frameworks: None | str | Literal['all'] | list[str] = None) -> dict[str, list[str]]: ...
    def object_detection(self, image: ContentT, *, model: str | None = None, threshold: float | None = None) -> list[ObjectDetectionOutputElement]: ...
    def question_answering(self, question: str, context: str, *, model: str | None = None, align_to_words: bool | None = None, doc_stride: int | None = None, handle_impossible_answer: bool | None = None, max_answer_len: int | None = None, max_question_len: int | None = None, max_seq_len: int | None = None, top_k: int | None = None) -> QuestionAnsweringOutputElement | list[QuestionAnsweringOutputElement]: ...
    def sentence_similarity(self, sentence: str, other_sentences: list[str], *, model: str | None = None) -> list[float]: ...
    def summarization(self, text: str, *, parameters: dict[str, Any] | None = None, model: str | None = None, clean_up_tokenization_spaces: bool | None = None, generate_parameters: dict[str, Any] | None = None, truncation: Literal['do_not_truncate', 'longest_first', 'only_first', 'only_second'] | None = None) -> SummarizationOutput: ...
    def table_question_answering(self, table: dict[str, Any], query: str, *, model: str | None = None, parameters: dict[str, Any] | None = None) -> TableQuestionAnsweringOutputElement: ...
    def tabular_classification(self, table: dict[str, Any], *, model: str | None = None) -> list[str]: ...
    def tabular_regression(self, table: dict[str, Any], *, model: str | None = None) -> list[float]: ...
    def text_classification(self, text: str, *, model: str | None = None, top_k: int | None = None, function_to_apply: TextClassificationOutputTransform | None = None) -> list[TextClassificationOutputElement]: ...
    @overload
    def text_generation(self, prompt: str, *, details: Literal[False] = ..., stream: Literal[False] = ..., model: str | None = None, adapter_id: str | None = None, best_of: int | None = None, decoder_input_details: bool | None = None, do_sample: bool | None = False, frequency_penalty: float | None = None, grammar: TextGenerationInputGrammarType | None = None, max_new_tokens: int | None = None, repetition_penalty: float | None = None, return_full_text: bool | None = False, seed: int | None = None, stop: list[str] | None = None, stop_sequences: list[str] | None = None, temperature: float | None = None, top_k: int | None = None, top_n_tokens: int | None = None, top_p: float | None = None, truncate: int | None = None, typical_p: float | None = None, watermark: bool | None = None) -> str: ...
    @overload
    def text_generation(self, prompt: str, *, details: Literal[True] = ..., stream: Literal[False] = ..., model: str | None = None, adapter_id: str | None = None, best_of: int | None = None, decoder_input_details: bool | None = None, do_sample: bool | None = False, frequency_penalty: float | None = None, grammar: TextGenerationInputGrammarType | None = None, max_new_tokens: int | None = None, repetition_penalty: float | None = None, return_full_text: bool | None = False, seed: int | None = None, stop: list[str] | None = None, stop_sequences: list[str] | None = None, temperature: float | None = None, top_k: int | None = None, top_n_tokens: int | None = None, top_p: float | None = None, truncate: int | None = None, typical_p: float | None = None, watermark: bool | None = None) -> TextGenerationOutput: ...
    @overload
    def text_generation(self, prompt: str, *, details: Literal[False] = ..., stream: Literal[True] = ..., model: str | None = None, adapter_id: str | None = None, best_of: int | None = None, decoder_input_details: bool | None = None, do_sample: bool | None = False, frequency_penalty: float | None = None, grammar: TextGenerationInputGrammarType | None = None, max_new_tokens: int | None = None, repetition_penalty: float | None = None, return_full_text: bool | None = False, seed: int | None = None, stop: list[str] | None = None, stop_sequences: list[str] | None = None, temperature: float | None = None, top_k: int | None = None, top_n_tokens: int | None = None, top_p: float | None = None, truncate: int | None = None, typical_p: float | None = None, watermark: bool | None = None) -> Iterable[str]: ...
    @overload
    def text_generation(self, prompt: str, *, details: Literal[True] = ..., stream: Literal[True] = ..., model: str | None = None, adapter_id: str | None = None, best_of: int | None = None, decoder_input_details: bool | None = None, do_sample: bool | None = False, frequency_penalty: float | None = None, grammar: TextGenerationInputGrammarType | None = None, max_new_tokens: int | None = None, repetition_penalty: float | None = None, return_full_text: bool | None = False, seed: int | None = None, stop: list[str] | None = None, stop_sequences: list[str] | None = None, temperature: float | None = None, top_k: int | None = None, top_n_tokens: int | None = None, top_p: float | None = None, truncate: int | None = None, typical_p: float | None = None, watermark: bool | None = None) -> Iterable[TextGenerationStreamOutput]: ...
    @overload
    def text_generation(self, prompt: str, *, details: Literal[True] = ..., stream: bool = ..., model: str | None = None, adapter_id: str | None = None, best_of: int | None = None, decoder_input_details: bool | None = None, do_sample: bool | None = False, frequency_penalty: float | None = None, grammar: TextGenerationInputGrammarType | None = None, max_new_tokens: int | None = None, repetition_penalty: float | None = None, return_full_text: bool | None = False, seed: int | None = None, stop: list[str] | None = None, stop_sequences: list[str] | None = None, temperature: float | None = None, top_k: int | None = None, top_n_tokens: int | None = None, top_p: float | None = None, truncate: int | None = None, typical_p: float | None = None, watermark: bool | None = None) -> TextGenerationOutput | Iterable[TextGenerationStreamOutput]: ...
    def text_to_image(self, prompt: str, *, negative_prompt: str | None = None, height: float | None = None, width: float | None = None, num_inference_steps: float | None = None, guidance_scale: float | None = None, model: str | None = None, scheduler: str | None = None, target_size: TextToImageTargetSize | None = None, seed: int | None = None, **kwargs) -> Image: ...
    def text_to_speech(self, text: str, *, model: str | None = None, do_sample: bool | None = None, early_stopping: bool | TextToSpeechEarlyStoppingEnum | None = None, epsilon_cutoff: float | None = None, eta_cutoff: float | None = None, max_length: int | None = None, max_new_tokens: int | None = None, min_length: int | None = None, min_new_tokens: int | None = None, num_beam_groups: int | None = None, num_beams: int | None = None, penalty_alpha: float | None = None, temperature: float | None = None, top_k: int | None = None, top_p: float | None = None, typical_p: float | None = None, use_cache: bool | None = None) -> bytes: ...
    def token_classification(self, text: str, *, model: str | None = None, aggregation_strategy: Literal['none', 'simple', 'first', 'average', 'max'] | None = None, ignore_labels: list[str] | None = None, stride: int | None = None) -> list[TokenClassificationOutputElement]: ...
    def translation(self, text: str, *, model: str | None = None, src_lang: str | None = None, tgt_lang: str | None = None, clean_up_tokenization_spaces: bool | None = None, truncation: Literal['do_not_truncate', 'longest_first', 'only_first', 'only_second'] | None = None, generate_parameters: dict[str, Any] | None = None) -> TranslationOutput: ...
    def visual_question_answering(self, image: ContentT, question: str, *, model: str | None = None, top_k: int | None = None) -> list[VisualQuestionAnsweringOutputElement]: ...
    def zero_shot_classification(self, text: str, labels: list[str], *, multi_label: bool = False, hypothesis_template: str | None = None, model: str | None = None) -> list[ZeroShotClassificationOutputElement]: ...
    def zero_shot_image_classification(self, image: ContentT, labels: list[str], *, model: str | None = None, hypothesis_template: str | None = None) -> list[ZeroShotImageClassificationOutputElement]: ...
    @staticmethod
    def get_recommended_model(task: str) -> str: ...
    def get_endpoint_info(self, *, model: str | None = None) -> dict[str, Any]: ...
    def health_check(self, model: str | None = None) -> bool: ...
    def get_model_status(self, model: str | None = None) -> ModelStatus: ...
    @property
    def chat(self) -> ProxyClientChat: ...

class _ProxyClient:
    def __init__(self, client: InferenceClient) -> None: ...

class ProxyClientChat(_ProxyClient):
    @property
    def completions(self) -> ProxyClientChatCompletions: ...

class ProxyClientChatCompletions(_ProxyClient):
    @property
    def create(self): ...
